install.packages("tidyverse")
install.packages("gcookbook")
install.packages(c("tidyverse","gcookbook"))
library(ggplot2)
source("C:/Users/Caleb Solomon/Documents/GitHub/School/UMD/2022-Fall-Semester/HGLO101/RAssignments/RFiles/CodingAssign2.R")
install.packages("RCPA3")
library(RCPA3)
welcome()
#An R Companion to Political Analysis 3rd edition
#Script for Chapter 1
# January 2023
# Birnir
##############################################################################################
rm(list=ls()) #Clearing workspace
##############################################################################################
#packages
library(RCPA3)
##############################################################################################
#Using data for analysis
#1.1 Interacting with r
#Computation with a mathematical operator (+)
2+2
#1.2 Objects (everything that exists whereas functions are everything that happens)
#with one value
pi
#With multiple values (vectors)
letters
LETTERS
#1.2.2 Creating objects
startValue <- 101 #equivalent to startvalue=101
endValue <- 150
#Objects with string
titleText = "Example of stored text"
#1.2.3. Accessing Some of an Object's Values
#Using square brackets
letters[10] #asks for the 10th letter in the alphabet
letters[1:10] #letters 1 through 10
states[,9] #values in the 9th column in the dataset states
states[5,1:5] #returns values 1-5 in the 5th row in the data states
#NOTE TYPO IN THE BOOK states[5, ] returns the 5th row states[1,] would return first row
#access a column/variable with $
states$biden2020 #states is the data biden2020 is the variable
#1.3 Functions - perform a definet set of actions
#one word no spaces followed by a bracket.  Arguments are the settings of a function.
#Functions have required and optional arguments
#Example install.packages() is a function and the only required argument is the name of the package, you could add optional
#arguments about where to install the packages etc. but those are not required.
#1.3.1  Using a function's arguments
#Use functions to complete tasks
#You can specify the values of a functions arguments, separating them with commas, inside the
#parentheses that follow the function's name
#Example the function seq(from, to, by) generates a sequence of numbers and has 3 arguments that allow you to specify
#the details of that sequence.
#You can find any function's generic syntax in the document file help(seq)
seq(1,49,3) #here R uses positional matching and this gives the same output as
seq(from=1, to=49, by=3) #where R uses keyword matching and this gives the same output as
seq(1, by=3, to=49) #where R uses a mixture of positional and keyword matching
seq(3,50,1)
seq(by=3, to=50, from=1)
seq(10, 2, from =1)
#functions ending with C are developed specifically for the companion i.e. crosstabC
#don't memorize functions - there are thousands - understand their logic
#1.3.2 Widget factory needs your help
#Play the widget factory game
widgetFactory()
#makeWidget(q, size, col, shape) #substitute in the generic the appropriate values
#Example
makeWidget(1,"small","blue","triangle")
library(RCPA3)
thisNumber <- 8
anotherNumber <- thisNumber / 4 ^ 3 * 2
nextNumber = sqrt(anotherNumber)
theAnswer = nextNumber + thisNumber
theAnswer
thisNumber
nextNumber
anotherNumber
theAnswer
plot(x)
plot(.)
plot(?)
plot(1)
source("~/.active-rstudio-document")
source("C:/Users/Caleb Solomon/Documents/GitHub/School/UMD/2023-Spring-Semester/GVPT201/HomeworkRScripts/HWK1/CalebSolomonHWK1.R")
#including the RCPA3 package
library(RCPA3)
#command execution for questions 3-5
thisNumber <- 8
anotherNumber <- thisNumber / 4 ^ 3 * 2
nextNumber <- sqrt(anotherNumber)
theAnswer <- nextNumber + thisNumber
#print each
thisNumber
anotherNumber
nextNumber
theAnswer
#plot attempts
#plot(x)
#plot(.)
#plot(?)
plot(1) #the minimum working plot
pnorm(1.475, mean = 0, sd=1, lower.tail = F)
library(RCPA3)
library(RCPA3)
library(RCPA3)
library(RCPA3)
pnorm(1.475, mean = 0, sd=1, lower.tail = F)
#5
p<-0.508
n<-7154.8
stderr<-sqrt(p*(1-p))/sqrt(n)
stderr
#6
CIprop(covid.made.lab, wt, nes, digits = 3, level = 95)
#8
testpropsC(x1=covid.made.lab, x2=0.500, w=wt, data=nes,
ci.level = 95,
response = "2. The novel coronavirus (COVID-19) was not developed intentionally in a lab")
pt(2.078, df=45, lower.tail = F)
#13
stddev<-22.832
n<-250
stderr = stddev/sqrt(n)
stderr
#14
(74.375-70)/stderr
help(pt)
#15
pt(tstat, df=249, lower.tail = F)
#15
pt((74.375-70)/stderr, df=249, lower.tail = F)
#18
r<-3
c<-3
df<-(r-1)*(c-1)
df
#19
qchisq((1-0.05), df=12)
#19
qchisq((1-0.05), df=4)
#20
# First row, chi-square stat = 8.351
df<-4
pchisq(8.351, 4, lower.tail = F)
# Second row
pchisq(12.823, 4, lower.tail = F)
# Second row
pchisq(12.823, 6, lower.tail = F)
# Third row
pchisq(17.046, 9, lower.tail = F)
# Fourth row
pchisq(16.525, 15, lower.tail = F)
# Fifth row
pchisq(20.236, 8, lower.tail = F)
#1
correlateC(x=list(world$ciaedex, world$fertility, world$income.tax.rate), stats = T)
# Caleb Solomon - HWK 8 R FILE
# Problem numbers refer to the ELMS numbers, not the respective workbook
# problem numbers.
library(RCPA3)
#1
correlateC(x=list(debate$issues, debate$integrity, debate$leadership, debate$empathy), stats = T)
#4
68.989 - (2.592*5)
#5
68.989 - (2.592*15)
help(regC)
(gini)
#8
gdp.percap.thou = world$gdp.percap / 1000
regC(formula = world$gini.index ~ gdp.percap.thou)
#14
biden2020.pct = states$biden2020 * 100
regC(formula = biden2020.pct ~ states$vep20.turnout)
regC(polity.score~rights.law.index, data = world)
#14
biden2020.pct = states$biden2020 * 100
regC(formula = biden2020.pct ~ states$vep20.turnout)
#14
biden2020.pct = states$biden2020 * 100
states$biden2020
biden2020.pct
#18
#Adjusted R-squared = 1-(((n-1)/(n-k-1))(1-Rsquared))
1-(((180-1)/(180-2-1))(1-0.577))
1-(((38-1)/(38-3-1))(1-0.104))
#18
#Adjusted R-squared = 1-(((n-1)/(n-k-1))*(1-Rsquared))
1-(((180-1)/(180-2-1))*(1-0.577))
1-(((38-1)/(38-3-1))*(1-0.104))
#Solved for Rsquared
((0.760-1)*(381-3-1))/(381-1)+1
#Solved for k
446-1-((446-1)*(1-0.093))/(1-0.072)
#19
#t-stat: partial regression coeff / se
# intercept
39.07/3.94
# x1
0.85/0.14
# x2
0.18/0.41
#21
60.892-2.273*15+12.611*1
#22
60.892-2.273*10+12.611*0
#31
regC(formula = states$vep20.turnout ~ states$region)
#32
# Midwest
71.656 - 2.431
# South
71.656 - 7.162
# West
71.656 - 3.202
#34
regC(formula = abortlaws ~ prochoice.percent + women.stateleg, data = states)
#35
mean.women.stateleg <- wtd.mean(states$women.stateleg)
plot(x=states$prochoice.percent, y=states$abortlaws,
cex=sqrt(states$women.stateleg/mean.min.wage))
#35
mean.women.stateleg <- wtd.mean(states$women.stateleg)
plot(x=states$prochoice.percent, y=states$abortlaws,
cex=sqrt(states$women.stateleg/mean.women.stateleg))
source("C:/Users/Caleb Solomon/Documents/GitHub/ROAR-LDT-Public/ntr/AgeMeasureAnalysis.R")
rm(list=ls())
library(tidyverse)
library(dplyr)
# Set directory to the root directory of the project folder
# Desktop
setwd("C:/Users/Caleb Solomon/Documents/GitHub/ROAR-LDT-Public")
# Grab the automated pg toolkit functions
load("ntr/automated-toolkit/version 1.1/automated toolkit/PG Toolkit v1.1.RData")
# Old versions
# source("C:/Users/Caleb Solomon/Documents/Github/ROAR-LDT-Public/ntr/automated-toolkit/automated-toolkit-OLD/PROCESSING-SCRIPTS.R")
# Laptop
# setwd("C:/Users/cjsol/Documents/GitHub/ROAR-LDT-Public")
# Load in original required data
metadata <- read.csv("data_allsubs/metadata_all_newcodes.csv")
long_newcodes_data <- read.csv("data_allsubs/LDT_alldata_long_newcodes.csv")
wide_newcodes_data <- read.csv("data_allsubs/LDT_alldata_wide_newcodes.csv")
### DEFINE USEFUL FUNCTIONS USED IN THE SCRIPT FILE BELOW ###
# Rounding function
# Takes in a list of elements, and a number of trailing decimal places to be
# specified. Rounds all values in the list accordingly, and removes extra
# whitespace in the elements of the list.
round_list <- function(list, digits) {
rounded_list <- lapply(list, function(elt) {
trimws(format(round(elt, digits), nsmall = digits))
})
return(rounded_list)
}
# Returns a data frame of data in the "long" format for the given age bin,
# where the data returned corresponds to ages in range (age_bin, age_bin + 1]
# Age data is retrieved from the passed in metadata file data frame.
get_long_bin_data <- function(metadata, long_data, age_bin) {
# Create the data frame
bin <- data.frame()
# Go through all subjects in the metadata
for (subject in metadata$subj) {
# If that subject is within the desired range
if (!is.na(metadata[subject, "visit_age"]) &&
as.numeric(metadata[subject, "visit_age"]) > (age_bin) &&
as.numeric(metadata[subject, "visit_age"]) <= age_bin + 1) {
# Add all of their data to the data frame
bin <- rbind(bin, subset(long_data,
visit_age == metadata[subject, "visit_age"])) # nolint
}
}
# Return the age bin data
return(bin)
}
# Takes in age data given by get_long_bin_data and the min and max age in
# the metadata-dataset. Outputs csv's with one-year age bin data within that
# range to the ~/age_data folder, named repsectively according to the convention
# above.
output_long_bin_data <- function(long_bin_data, metadata, min_age, max_age) {
# Create a directory for storing the age data
if (!file.exists("ntr/age_data")) {
dir.create("ntr/age_data")
}
# For each age bin in the range passed in
for (age in seq(from = ceiling(min_age), to = ceiling(max_age), by = 1)) {
# Get the relevant data for that age bin
temp_df <- get_long_bin_data(metadata, long_bin_data, age - 1)
# Set the write path to be named corresponding to the age bin
out_path <- paste0("ntr/age_data/bin_", age - 1, ".csv")
# Write the data to the file
write.csv(temp_df, file = out_path)
}
}
# Returns a data frame whose words are rows/pseudowords and whose columns are
# different age bins. Each entry is the average for the respective statistic
# (as determined by the value passed to "data_type").
# Takes in data in the "long" format, metadata for subject ages, a data frame
# of word statistics based on the wordStatistics csv where each word is
# listed once, and the minimum and maximum age of the subjects in the data set.
# The first parameter denotes the type of data to be returned; right now,
# accuracy and response time are supported (denoted "acc" and "rt" in accordance
# with the names of the columns.
get_avg_word_age_data <- function(data_type, long_data, metadata, word_statistics, min_age, max_age) {
# Create a vector of age bins from the min_age to the max_age of the dataset
age_bins <- (ceiling(min_age)-1):(ceiling(max_age)-1)
# Create an empty data frame with words as rows and age bins as columns
word_age_averages <- data.frame(word = word_statistics$STRING,
stringsAsFactors = FALSE)
# Add columns for each age bin and initialize them with empty values
for (age in age_bins) {
bin_col <- as.character(age)
word_age_averages[, bin_col] <- NA
}
# Progress counters:
completed <- 1
word_count <- nrow(word_statistics)
# Iterate through all of the words, now in word_age_averages, and,
# iterating through all instances of that word in a given age bin, sum the
# accuracies (0/1) and ultimately divide by the number of occurrences
# to obtain the average for that word for that age bin.
for (word in word_age_averages$word) {
for (age in age_bins) {
# Get the data for that age
age_data <- get_long_bin_data(metadata, long_data, age)
# Only operate if there is data to operate on (for efficiency)
# TODO: Allow for any data to be gleaned from the set as long as
# the passed in column name (to the data_type parameter) is valid,
# and its entries are numbers.
# For now, only accuracy and response time are supported.
if (nrow(age_data) != 0) {
# Get all the relevant statistics for the given word
if (data_type == "acc") {
temp <- age_data$acc[age_data$word == word]
} else if (data_type == "rt") {
temp <- age_data$rt[age_data$word == word]
}
# Set the average in word_age_averages based on the bin
# This will throw a warning when an element of accs is NA, so
# ignore it
suppressWarnings({
word_age_averages[word_age_averages$word == word, as.character(age)] <- mean(temp)
})
}
# Print progress at the end of each age bin:
message <- sprintf("IN PROGRESS: Age bin %3d of %3d in word %3d of %3d.\r",
age - 5, ncol(word_age_averages) - 1, completed, word_count)
cat(message)
}
# Update number of words completed
completed <- completed + 1
}
# Progress result
cat("\n")
cat("Completed ", completed - 1, " of ", word_count, " words.\n")
# Ultimately return the averages
return(word_age_averages)
}
### SCRIPTING ###
# Changing metadata to reflect age in years
updated_metadata <- filter(metadata, !is.na(visit_age))
updated_metadata[2] <- updated_metadata[2]/12
# USE THIS FOR ROUNDED AGE DATA IN PLACE OF ABOVE LINE
# updated_metadata[2] <- round_list(updated_metadata[2]/12, 2)
# Get the max and min age from the data we have
min_age <- min(as.numeric(updated_metadata$visit_age))
max_age <- max(as.numeric(updated_metadata$visit_age))
# Remove all entries from long_newcodes_data associated with subjects whose
# ages are unknown (i.e. only keeps rows in updated_long_newcodes whose "subj"
# entry is also present in the updated_metadata)
updated_long_newcodes <- long_newcodes_data %>%
filter(subj %in% updated_metadata$subj)
# For wide_newcodes_data, first prepend a column with all subject IDs (the
# data is already sorted in ascending subject ID order). Then filter.
# Note that there are only 120 (non-outlier) subjects in the data.
subj <- 1:120
updated_wide_newcodes <- data.frame(subj, wide_newcodes_data) %>%
filter(subj %in% updated_metadata$subj)
# Add age data to the updated_long_newcodes next to each subject
updated_long_newcodes <- merge(updated_long_newcodes,
updated_metadata[, c("subj", "visit_age")], by = "subj", all.x = TRUE)
# Output all of the long age bin data to some files, named by the convention
# ~/age_data/bin_<age_bin>.csv
output_long_bin_data(updated_long_newcodes, updated_metadata, min_age, max_age)
# Next, we can compute the averages for each word for each age bin,
# based on all of the data across age bins
# Read in the word statistics
word_statistics <- read.csv("data_allsubs/wordStatistics.csv")
# Get the accuracies for each word and age group.
average_word_age_accuracies <- get_avg_word_age_data("acc", updated_long_newcodes, updated_metadata, word_statistics, min_age, max_age)
# Get the average response times for each word and age group
average_word_age_rt <- get_avg_word_age_data("rt", updated_long_newcodes, updated_metadata, word_statistics, min_age, max_age)
# Write them both to a csv
write.csv(average_word_age_accuracies, "ntr/age_data/average_word_age_accuracies.csv")
write.csv(average_word_age_rt, "ntr/age_data/average_word_age_rt.csv")
# Next we are interested in getting the average accuracy and response time data
# for the different age bins for all words corresponding to a certain
# phoneme/grapheme in a position, onset/rime, etc.
# EXAMPLE: Get all the words with a certain phoneme/grapheme
# Returns the subset of scoredPGwords whose corresponding words are present in the provided subset
# This is super inefficient but the function itself should work.
filter_subset <- function(scoredPGwords, subset) {
filtered_scored_words <- list()
for (data in scoredPGwords) {
for (i in 1:nrow(subset["STRING"])) {
if (data["spelling",][1] %in% subset["STRING"][i,]) {
print(i)
filtered_scored_words <- append(filtered_scored_words, data)
break
}
}
}
return(filtered_scored_words)
}
retdata <- filter_subset(scored_words_PG, word_statistics)
View(retdata)
View(retdata[1])
source("C:/Users/Caleb Solomon/Documents/GitHub/ROAR-LDT-Public/ntr/AgeMeasureAnalysis.R")
ret <- word_pattern(scored_words_PG, phoneme = "any", grapheme = "int", position = "wf")
# Get a list of all the words with a certain phoneme/grapheme
ret <- word_pattern(scored_words_PG, phoneme = "any", grapheme = "int", position = "wf")
View(scored_words_PG)
View(scored_words_PG[128])
View(scored_words_PG[[128]])
get_scored_words <- function(scored_list, word) {
for (df in scored_list) {
if (any(df == word, arr.ind = TRUE)) {
return(c(df))
}
}
return(NULL)  # Return NULL if word not found in any data frame
}
# Usage
result <- get_scored_words(scored_words_PG, "flint")
View(result)
summarize_words(get_scored_words(scored_words_OR, "flint"), "OR")
# Filter so that we only get the words that are also part of the ROAR dataset
# summarize them
summarize_roar_words <- function(scored_words, phoneme, grapheme, position, roar_words) {
# Takes in a list of scored words, a phoneme, a grapheme, and a position for them
# Outputs all words in the ROAR corpus that have been processed by the PG Toolkit
match <- word_pattern(scored_words, phoneme, grapheme, position)
wordlist <- match[match %in% roar_words]
if (length(wordlist)==0) {
print("No matching words with the given inputs are present in the ROAR dataset.")
} else {
summarize_words(get_scored_words(scored_words, wordlist))
}
}
# Right now this confirmed works for scored_words_OR. Should work for scored_words_PG as well.
get_scored_words <- function(scored_words, words) {
ret <- list()
View(words)
for (i in length(scored_words)) {
if (scored_words[[i]]["spelling", 1] %in% words) {
print("inside the if")
ret <- c(ret, list(scored_words[[i]]))
}
}
return(ret)
}
summarize_roar_words(scored_words_OR, "any", "en", "wf", word_statistics$STRING)
# GOALS:
# Get a list of all words corresponding to a certain phoneme/grapheme mapping
# Filter so that we only get the words that are also part of the ROAR dataset
# summarize them
summarize_roar_words <- function(scored_words, phoneme, grapheme, position, roar_words) {
# Takes in a list of scored words, a phoneme, a grapheme, and a position for them
# Outputs all words in the ROAR corpus that have been processed by the PG Toolkit
match <- word_pattern(scored_words, phoneme, grapheme, position)
wordlist <- match[match %in% roar_words]
if (length(wordlist)==0) {
print("No matching words with the given inputs are present in the ROAR dataset.")
} else {
summarize_words(get_scored_words(scored_words, wordlist))
}
}
# Right now this confirmed works for scored_words_OR. Should work for scored_words_PG as well.
get_scored_words <- function(scored_words, words) {
ret <- list()
for (i in 1:length(scored_words)) {
if (scored_words[[i]]["spelling", 1] %in% words) {
ret <- append(ret, list(scored_words[[i]]))
}
}
View(ret)
return(ret)
}
summarize_roar_words(scored_words_OR, "any", "en", "wf", word_statistics$STRING)
View(ret[[1]])
